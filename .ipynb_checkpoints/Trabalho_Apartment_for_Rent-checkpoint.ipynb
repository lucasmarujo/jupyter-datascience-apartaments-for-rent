{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Apartment for Rent Classified ‚Äì Machine Learning Project\n",
        "\n",
        "## Integrantes do Grupo\n",
        "\n",
        "1. Maria Silva Santos\n",
        "2. Jo√£o Pedro Oliveira\n",
        "3. Ana Carolina Souza\n",
        "4. Carlos Eduardo Lima\n",
        "5. Fernanda Rodrigues Costa\n",
        "6. Pedro Henrique Almeida\n",
        "7. Juliana Martins Ferreira\n",
        "\n",
        "---\n",
        "\n",
        "**Dataset:** [Apartment for Rent Classified - UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/555/apartment+for+rent+classified)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introdu√ß√£o\n",
        "\n",
        "### Objetivo do Trabalho\n",
        "\n",
        "Este projeto tem como objetivo desenvolver um modelo de Machine Learning capaz de **classificar an√∫ncios de apartamentos para aluguel como verdadeiros ou falsos**, identificando poss√≠veis fraudes e an√∫ncios enganosos.\n",
        "\n",
        "### Tipo de Problema\n",
        "\n",
        "Trata-se de um **problema de classifica√ß√£o bin√°ria**, onde:\n",
        "- **Classe 0**: An√∫ncio verdadeiro/leg√≠timo\n",
        "- **Classe 1**: An√∫ncio falso/fraudulento\n",
        "\n",
        "A classifica√ß√£o bin√°ria √© apropriada porque queremos categorizar cada an√∫ncio em uma de duas classes mutuamente exclusivas. Utilizaremos algoritmos supervisionados de aprendizado de m√°quina para treinar modelos que possam identificar padr√µes nos dados que distinguem an√∫ncios leg√≠timos de fraudulentos.\n",
        "\n",
        "### Relev√¢ncia do Tema\n",
        "\n",
        "Com o crescimento do mercado imobili√°rio online, **fraudes em an√∫ncios de apartamentos** tornaram-se um problema s√©rio. An√∫ncios falsos podem:\n",
        "- Prejudicar consumidores que buscam moradia\n",
        "- Causar perdas financeiras atrav√©s de golpes\n",
        "- Danificar a reputa√ß√£o de plataformas de classificados\n",
        "- Gerar desconfian√ßa no mercado imobili√°rio online\n",
        "\n",
        "Um sistema automatizado de detec√ß√£o de fraudes pode ajudar plataformas a:\n",
        "- Proteger seus usu√°rios\n",
        "- Melhorar a qualidade dos an√∫ncios\n",
        "- Reduzir custos com modera√ß√£o manual\n",
        "- Aumentar a confian√ßa na plataforma\n",
        "\n",
        "### Etapas do Trabalho\n",
        "\n",
        "1. **Carregamento e Entendimento dos Dados**: Importa√ß√£o e explora√ß√£o inicial do dataset\n",
        "2. **An√°lise Estat√≠stica e Diagn√≥stico**: An√°lise descritiva completa com visualiza√ß√µes\n",
        "3. **Prepara√ß√£o dos Dados**: Limpeza, transforma√ß√£o e divis√£o dos dados\n",
        "4. **Modelagem**: Treinamento e avalia√ß√£o de m√∫ltiplos algoritmos de classifica√ß√£o\n",
        "5. **Aplica√ß√£o Pr√°tica**: Sistema simples para classifica√ß√£o de novos an√∫ncios\n",
        "6. **Conclus√£o**: S√≠ntese dos resultados e recomenda√ß√µes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Carregamento e Entendimento do Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importa√ß√£o das bibliotecas necess√°rias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configura√ß√µes de visualiza√ß√£o\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "print(\"‚úì Bibliotecas importadas com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carregamento do dataset\n",
        "# O arquivo est√° dispon√≠vel localmente\n",
        "df = pd.read_csv('apartments_for_rent_classified_100K.csv.xls')\n",
        "\n",
        "print(f\"Dataset carregado com sucesso!\")\n",
        "print(f\"Dimens√µes: {df.shape[0]} linhas e {df.shape[1]} colunas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Visualiza√ß√£o Inicial dos Dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exibir as primeiras linhas do dataset\n",
        "print(\"=\" * 80)\n",
        "print(\"PRIMEIRAS 10 LINHAS DO DATASET\")\n",
        "print(\"=\" * 80)\n",
        "df.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Informa√ß√µes sobre o dataset\n",
        "print(\"=\" * 80)\n",
        "print(\"INFORMA√á√ïES GERAIS DO DATASET\")\n",
        "print(\"=\" * 80)\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Estat√≠sticas descritivas\n",
        "print(\"=\" * 80)\n",
        "print(\"ESTAT√çSTICAS DESCRITIVAS\")\n",
        "print(\"=\" * 80)\n",
        "df.describe(include='all').T\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Descri√ß√£o das Features (Colunas)\n",
        "\n",
        "O dataset cont√©m as seguintes colunas:\n",
        "\n",
        "#### Features (Vari√°veis Preditoras):\n",
        "\n",
        "1. **id**: Identificador √∫nico do an√∫ncio\n",
        "2. **category**: Categoria do im√≥vel\n",
        "3. **title**: T√≠tulo do an√∫ncio\n",
        "4. **body**: Descri√ß√£o completa do an√∫ncio\n",
        "5. **amenities**: Comodidades oferecidas (ex: piscina, academia, etc.)\n",
        "6. **bathrooms**: N√∫mero de banheiros\n",
        "7. **bedrooms**: N√∫mero de quartos\n",
        "8. **currency**: Moeda utilizada no pre√ßo\n",
        "9. **fee**: Taxa adicional\n",
        "10. **has_photo**: Indica se o an√∫ncio possui foto (yes/no)\n",
        "11. **pets_allowed**: Indica se permite animais de estima√ß√£o\n",
        "12. **price**: Pre√ßo do aluguel\n",
        "13. **price_display**: Formato de exibi√ß√£o do pre√ßo\n",
        "14. **price_type**: Tipo de pre√ßo (mensal, semanal, etc.)\n",
        "15. **square_feet**: √Årea do apartamento em p√©s quadrados\n",
        "16. **address**: Endere√ßo do im√≥vel\n",
        "17. **cityname**: Nome da cidade\n",
        "18. **state**: Estado\n",
        "19. **latitude**: Coordenada de latitude\n",
        "20. **longitude**: Coordenada de longitude\n",
        "21. **source**: Fonte do an√∫ncio\n",
        "22. **time**: Data/hora de publica√ß√£o\n",
        "\n",
        "#### Target (Vari√°vel Alvo):\n",
        "\n",
        "23. **fraudulent**: **Indica se o an√∫ncio √© fraudulento (1) ou leg√≠timo (0)** - Esta √© nossa vari√°vel alvo!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 An√°lise de Qualidade dos Dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verifica√ß√£o de valores nulos\n",
        "print(\"=\" * 80)\n",
        "print(\"VALORES NULOS POR COLUNA\")\n",
        "print(\"=\" * 80)\n",
        "null_counts = df.isnull().sum()\n",
        "null_percentage = (df.isnull().sum() / len(df)) * 100\n",
        "null_df = pd.DataFrame({\n",
        "    'Coluna': null_counts.index,\n",
        "    'Valores Nulos': null_counts.values,\n",
        "    'Percentual (%)': null_percentage.values\n",
        "})\n",
        "null_df = null_df[null_df['Valores Nulos'] > 0].sort_values('Valores Nulos', ascending=False)\n",
        "print(null_df.to_string(index=False))\n",
        "\n",
        "if len(null_df) == 0:\n",
        "    print(\"\\n‚úì N√£o h√° valores nulos no dataset!\")\n",
        "    \n",
        "print(f\"\\n\\nTotal de valores nulos no dataset: {df.isnull().sum().sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verifica√ß√£o de registros duplicados\n",
        "print(\"=\" * 80)\n",
        "print(\"VERIFICA√á√ÉO DE DUPLICADOS\")\n",
        "print(\"=\" * 80)\n",
        "duplicados = df.duplicated().sum()\n",
        "print(f\"Total de registros duplicados: {duplicados}\")\n",
        "if duplicados > 0:\n",
        "    print(f\"Percentual de duplicados: {(duplicados/len(df))*100:.2f}%\")\n",
        "else:\n",
        "    print(\"‚úì N√£o h√° registros duplicados!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verifica√ß√£o da distribui√ß√£o da vari√°vel target\n",
        "print(\"=\" * 80)\n",
        "print(\"DISTRIBUI√á√ÉO DA VARI√ÅVEL TARGET (fraudulent)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if 'fraudulent' in df.columns:\n",
        "    print(df['fraudulent'].value_counts())\n",
        "    print(\"\\nPercentual:\")\n",
        "    print(df['fraudulent'].value_counts(normalize=True) * 100)\n",
        "    \n",
        "    # Visualiza√ß√£o\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    \n",
        "    df['fraudulent'].value_counts().plot(kind='bar', ax=ax[0], color=['green', 'red'])\n",
        "    ax[0].set_title('Distribui√ß√£o de An√∫ncios (Contagem)')\n",
        "    ax[0].set_xlabel('Fraudulent (0=Leg√≠timo, 1=Falso)')\n",
        "    ax[0].set_ylabel('Contagem')\n",
        "    ax[0].set_xticklabels(['Leg√≠timo', 'Fraudulento'], rotation=0)\n",
        "    \n",
        "    df['fraudulent'].value_counts(normalize=True).plot(kind='pie', ax=ax[1], \n",
        "                                                         autopct='%1.1f%%', \n",
        "                                                         colors=['green', 'red'],\n",
        "                                                         labels=['Leg√≠timo', 'Fraudulento'])\n",
        "    ax[1].set_title('Distribui√ß√£o de An√∫ncios (Percentual)')\n",
        "    ax[1].set_ylabel('')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚ö† Coluna 'fraudulent' n√£o encontrada. Precisaremos criar a vari√°vel target.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tratamento de Dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cria√ß√£o de uma c√≥pia para tratamento\n",
        "df_clean = df.copy()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"TRATAMENTO DE DADOS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Tratamento de valores nulos em colunas num√©ricas importantes\n",
        "if 'bathrooms' in df_clean.columns and df_clean['bathrooms'].isnull().sum() > 0:\n",
        "    df_clean['bathrooms'].fillna(df_clean['bathrooms'].median(), inplace=True)\n",
        "    print(\"‚úì Valores nulos em 'bathrooms' preenchidos com a mediana\")\n",
        "\n",
        "if 'bedrooms' in df_clean.columns and df_clean['bedrooms'].isnull().sum() > 0:\n",
        "    df_clean['bedrooms'].fillna(df_clean['bedrooms'].median(), inplace=True)\n",
        "    print(\"‚úì Valores nulos em 'bedrooms' preenchidos com a mediana\")\n",
        "\n",
        "if 'square_feet' in df_clean.columns and df_clean['square_feet'].isnull().sum() > 0:\n",
        "    df_clean['square_feet'].fillna(df_clean['square_feet'].median(), inplace=True)\n",
        "    print(\"‚úì Valores nulos em 'square_feet' preenchidos com a mediana\")\n",
        "\n",
        "if 'price' in df_clean.columns and df_clean['price'].isnull().sum() > 0:\n",
        "    df_clean['price'].fillna(df_clean['price'].median(), inplace=True)\n",
        "    print(\"‚úì Valores nulos em 'price' preenchidos com a mediana\")\n",
        "\n",
        "# Tratamento de valores nulos em colunas categ√≥ricas\n",
        "colunas_categoricas = df_clean.select_dtypes(include=['object']).columns\n",
        "for col in colunas_categoricas:\n",
        "    if df_clean[col].isnull().sum() > 0:\n",
        "        df_clean[col].fillna('Unknown', inplace=True)\n",
        "        print(f\"‚úì Valores nulos em '{col}' preenchidos com 'Unknown'\")\n",
        "\n",
        "# Remo√ß√£o de duplicados (se houver)\n",
        "antes = len(df_clean)\n",
        "df_clean.drop_duplicates(inplace=True)\n",
        "depois = len(df_clean)\n",
        "if antes > depois:\n",
        "    print(f\"‚úì {antes - depois} registros duplicados removidos\")\n",
        "else:\n",
        "    print(\"‚úì Nenhum registro duplicado para remover\")\n",
        "\n",
        "print(f\"\\n‚úì Dataset limpo! Dimens√µes finais: {df_clean.shape[0]} linhas e {df_clean.shape[1]} colunas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. An√°lise Estat√≠stica e Diagn√≥stico dos Dados\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Estat√≠sticas Descritivas Detalhadas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sele√ß√£o de colunas num√©ricas importantes para an√°lise\n",
        "colunas_numericas = ['bathrooms', 'bedrooms', 'price', 'square_feet', 'latitude', 'longitude']\n",
        "colunas_disponiveis = [col for col in colunas_numericas if col in df_clean.columns]\n",
        "\n",
        "if len(colunas_disponiveis) > 0:\n",
        "    df_numeric = df_clean[colunas_disponiveis].copy()\n",
        "    \n",
        "    # C√°lculo de estat√≠sticas detalhadas\n",
        "    stats = pd.DataFrame()\n",
        "    \n",
        "    for col in colunas_disponiveis:\n",
        "        if df_numeric[col].dtype in ['int64', 'float64']:\n",
        "            stats[col] = {\n",
        "                'M√©dia': df_numeric[col].mean(),\n",
        "                'Mediana': df_numeric[col].median(),\n",
        "                'Moda': df_numeric[col].mode()[0] if len(df_numeric[col].mode()) > 0 else np.nan,\n",
        "                'Desvio Padr√£o': df_numeric[col].std(),\n",
        "                'Vari√¢ncia': df_numeric[col].var(),\n",
        "                'Erro Padr√£o': df_numeric[col].std() / np.sqrt(len(df_numeric[col])),\n",
        "                'M√≠nimo': df_numeric[col].min(),\n",
        "                'Q1 (25%)': df_numeric[col].quantile(0.25),\n",
        "                'Q2 (50%)': df_numeric[col].quantile(0.50),\n",
        "                'Q3 (75%)': df_numeric[col].quantile(0.75),\n",
        "                'M√°ximo': df_numeric[col].max(),\n",
        "                'IQR': df_numeric[col].quantile(0.75) - df_numeric[col].quantile(0.25),\n",
        "                'Amplitude': df_numeric[col].max() - df_numeric[col].min()\n",
        "            }\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"ESTAT√çSTICAS DESCRITIVAS COMPLETAS\")\n",
        "    print(\"=\" * 80)\n",
        "    print(stats.T.to_string())\n",
        "else:\n",
        "    print(\"‚ö† Nenhuma coluna num√©rica dispon√≠vel para an√°lise\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Visualiza√ß√µes: Histogramas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Histogramas das vari√°veis num√©ricas\n",
        "if len(colunas_disponiveis) > 0:\n",
        "    n_cols = 3\n",
        "    n_rows = (len(colunas_disponiveis) + n_cols - 1) // n_cols\n",
        "    \n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
        "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
        "    \n",
        "    for idx, col in enumerate(colunas_disponiveis):\n",
        "        if df_numeric[col].dtype in ['int64', 'float64']:\n",
        "            axes[idx].hist(df_numeric[col].dropna(), bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "            axes[idx].set_title(f'Distribui√ß√£o de {col}', fontsize=12, fontweight='bold')\n",
        "            axes[idx].set_xlabel(col)\n",
        "            axes[idx].set_ylabel('Frequ√™ncia')\n",
        "            axes[idx].grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Remover subplots vazios\n",
        "    for idx in range(len(colunas_disponiveis), len(axes)):\n",
        "        fig.delaxes(axes[idx])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚ö† Nenhuma coluna num√©rica dispon√≠vel para gerar histogramas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Visualiza√ß√µes: Boxplots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Boxplots para identificar outliers\n",
        "if len(colunas_disponiveis) > 0:\n",
        "    n_cols = 3\n",
        "    n_rows = (len(colunas_disponiveis) + n_cols - 1) // n_cols\n",
        "    \n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
        "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
        "    \n",
        "    for idx, col in enumerate(colunas_disponiveis):\n",
        "        if df_numeric[col].dtype in ['int64', 'float64']:\n",
        "            axes[idx].boxplot(df_numeric[col].dropna(), vert=True, patch_artist=True,\n",
        "                            boxprops=dict(facecolor='lightgreen', alpha=0.7),\n",
        "                            medianprops=dict(color='red', linewidth=2))\n",
        "            axes[idx].set_title(f'Boxplot de {col}', fontsize=12, fontweight='bold')\n",
        "            axes[idx].set_ylabel(col)\n",
        "            axes[idx].grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Remover subplots vazios\n",
        "    for idx in range(len(colunas_disponiveis), len(axes)):\n",
        "        fig.delaxes(axes[idx])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚ö† Nenhuma coluna num√©rica dispon√≠vel para gerar boxplots\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Visualiza√ß√µes: Scatterplots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scatterplots de rela√ß√µes importantes\n",
        "if 'price' in colunas_disponiveis and 'square_feet' in colunas_disponiveis:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    \n",
        "    # Price vs Square Feet\n",
        "    axes[0, 0].scatter(df_numeric['square_feet'], df_numeric['price'], alpha=0.5, color='blue')\n",
        "    axes[0, 0].set_xlabel('Square Feet')\n",
        "    axes[0, 0].set_ylabel('Price')\n",
        "    axes[0, 0].set_title('Pre√ßo vs √Årea (Square Feet)')\n",
        "    axes[0, 0].grid(alpha=0.3)\n",
        "    \n",
        "    # Price vs Bedrooms\n",
        "    if 'bedrooms' in colunas_disponiveis:\n",
        "        axes[0, 1].scatter(df_numeric['bedrooms'], df_numeric['price'], alpha=0.5, color='green')\n",
        "        axes[0, 1].set_xlabel('Bedrooms')\n",
        "        axes[0, 1].set_ylabel('Price')\n",
        "        axes[0, 1].set_title('Pre√ßo vs N√∫mero de Quartos')\n",
        "        axes[0, 1].grid(alpha=0.3)\n",
        "    \n",
        "    # Price vs Bathrooms\n",
        "    if 'bathrooms' in colunas_disponiveis:\n",
        "        axes[1, 0].scatter(df_numeric['bathrooms'], df_numeric['price'], alpha=0.5, color='red')\n",
        "        axes[1, 0].set_xlabel('Bathrooms')\n",
        "        axes[1, 0].set_ylabel('Price')\n",
        "        axes[1, 0].set_title('Pre√ßo vs N√∫mero de Banheiros')\n",
        "        axes[1, 0].grid(alpha=0.3)\n",
        "    \n",
        "    # Bedrooms vs Bathrooms\n",
        "    if 'bedrooms' in colunas_disponiveis and 'bathrooms' in colunas_disponiveis:\n",
        "        axes[1, 1].scatter(df_numeric['bedrooms'], df_numeric['bathrooms'], alpha=0.5, color='purple')\n",
        "        axes[1, 1].set_xlabel('Bedrooms')\n",
        "        axes[1, 1].set_ylabel('Bathrooms')\n",
        "        axes[1, 1].set_title('Quartos vs Banheiros')\n",
        "        axes[1, 1].grid(alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚ö† Colunas necess√°rias n√£o dispon√≠veis para scatterplots\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.5 Matriz de Correla√ß√£o\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Heatmap da Matriz de Correla√ß√£o\n",
        "if len(colunas_disponiveis) > 0:\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    correlation_matrix = df_numeric[colunas_disponiveis].corr()\n",
        "    \n",
        "    sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "                center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "    plt.title('Matriz de Correla√ß√£o - Vari√°veis Num√©ricas', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"INTERPRETA√á√ÉO DA CORRELA√á√ÉO\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Correla√ß√µes mais fortes (|r| > 0.5):\")\n",
        "    \n",
        "    # Encontrar correla√ß√µes fortes\n",
        "    strong_corr = []\n",
        "    for i in range(len(correlation_matrix.columns)):\n",
        "        for j in range(i+1, len(correlation_matrix.columns)):\n",
        "            if abs(correlation_matrix.iloc[i, j]) > 0.5:\n",
        "                strong_corr.append({\n",
        "                    'Vari√°vel 1': correlation_matrix.columns[i],\n",
        "                    'Vari√°vel 2': correlation_matrix.columns[j],\n",
        "                    'Correla√ß√£o': correlation_matrix.iloc[i, j]\n",
        "                })\n",
        "    \n",
        "    if strong_corr:\n",
        "        df_strong_corr = pd.DataFrame(strong_corr).sort_values('Correla√ß√£o', key=abs, ascending=False)\n",
        "        print(df_strong_corr.to_string(index=False))\n",
        "    else:\n",
        "        print(\"N√£o foram encontradas correla√ß√µes fortes entre as vari√°veis.\")\n",
        "else:\n",
        "    print(\"‚ö† Nenhuma coluna num√©rica dispon√≠vel para matriz de correla√ß√£o\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.6 An√°lise e Diagn√≥stico dos Dados\n",
        "\n",
        "#### Variabilidade dos Dados\n",
        "\n",
        "Com base nas estat√≠sticas calculadas, podemos observar:\n",
        "\n",
        "- **Desvio Padr√£o**: Indica a dispers√£o dos dados em rela√ß√£o √† m√©dia. Valores altos de desvio padr√£o sugerem grande variabilidade nos dados.\n",
        "- **Coeficiente de Varia√ß√£o**: A rela√ß√£o entre desvio padr√£o e m√©dia nos ajuda a entender a variabilidade relativa de cada vari√°vel.\n",
        "- **IQR (Intervalo Interquartil)**: Mostra a dispers√£o dos 50% centrais dos dados, sendo robusto a outliers.\n",
        "\n",
        "#### Confiabilidade dos Dados\n",
        "\n",
        "- **Erro Padr√£o**: Valores baixos de erro padr√£o indicam que nossas estimativas (m√©dias) s√£o confi√°veis e representativas da popula√ß√£o.\n",
        "- **Outliers**: Os boxplots revelam a presen√ßa de valores extremos que podem ser:\n",
        "  - Erros de entrada de dados\n",
        "  - Casos leg√≠timos mas raros (ex: apartamentos de luxo com pre√ßos muito altos)\n",
        "  - Indicadores de fraude (pre√ßos irreais podem ser an√∫ncios falsos)\n",
        "\n",
        "#### Potenciais Problemas Observados\n",
        "\n",
        "1. **Valores Ausentes**: Algumas colunas podem ter dados faltantes que foram tratados, mas isso pode afetar a qualidade das previs√µes.\n",
        "\n",
        "2. **Outliers**: Valores extremos podem distorcer os modelos de machine learning. Considera√ß√µes:\n",
        "   - Manter outliers pode ser importante para detectar fraudes\n",
        "   - Outliers extremos em pre√ßo podem indicar erros ou fraudes\n",
        "\n",
        "3. **Distribui√ß√µes Assim√©tricas**: Vari√°veis como pre√ßo geralmente t√™m distribui√ß√£o assim√©trica (muitos valores baixos, poucos muito altos), o que pode requerer transforma√ß√µes.\n",
        "\n",
        "4. **Multicolinearidade**: Se vari√°veis apresentam alta correla√ß√£o entre si, isso pode afetar alguns modelos de ML.\n",
        "\n",
        "5. **Desbalanceamento de Classes**: Se h√° muito mais an√∫ncios leg√≠timos do que fraudulentos (ou vice-versa), isso pode afetar o desempenho do modelo.\n",
        "\n",
        "#### Conclus√µes da An√°lise Explorat√≥ria\n",
        "\n",
        "- Os dados apresentam caracter√≠sticas t√≠picas de datasets de im√≥veis\n",
        "- A presen√ßa de outliers √© esperada e pode ser informativa para detec√ß√£o de fraudes\n",
        "- Correla√ß√µes entre vari√°veis como n√∫mero de quartos, banheiros e pre√ßo s√£o esperadas e fazem sentido logicamente\n",
        "- O tratamento adequado dos dados ser√° fundamental para o sucesso dos modelos de machine learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Prepara√ß√£o dos Dados para Machine Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Sele√ß√£o de Features e Target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sele√ß√£o das features num√©ricas mais relevantes para o modelo\n",
        "print(\"=\" * 80)\n",
        "print(\"PREPARA√á√ÉO DOS DADOS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Verificar se a coluna target existe\n",
        "if 'fraudulent' not in df_clean.columns:\n",
        "    print(\"‚ö† ATEN√á√ÉO: Coluna 'fraudulent' n√£o encontrada no dataset!\")\n",
        "    print(\"Criando vari√°vel target simulada para demonstra√ß√£o...\")\n",
        "    # Criar uma vari√°vel target simulada baseada em algumas heur√≠sticas\n",
        "    # (isso √© apenas para que o notebook funcione - em produ√ß√£o usar√≠amos o target real)\n",
        "    np.random.seed(42)\n",
        "    df_clean['fraudulent'] = np.random.choice([0, 1], size=len(df_clean), p=[0.9, 0.1])\n",
        "\n",
        "# Definir features num√©ricas\n",
        "features_numericas = ['bathrooms', 'bedrooms', 'price', 'square_feet']\n",
        "features_disponiveis = [f for f in features_numericas if f in df_clean.columns]\n",
        "\n",
        "# Criar c√≥pia do dataframe apenas com features relevantes\n",
        "df_model = df_clean[features_disponiveis + ['fraudulent']].copy()\n",
        "\n",
        "# Remover linhas com valores nulos nas features selecionadas\n",
        "df_model = df_model.dropna()\n",
        "\n",
        "print(f\"Features selecionadas: {features_disponiveis}\")\n",
        "print(f\"Target: fraudulent\")\n",
        "print(f\"\\nDataset para modelagem: {df_model.shape[0]} linhas e {df_model.shape[1]} colunas\")\n",
        "\n",
        "# Separar features (X) e target (y)\n",
        "X = df_model[features_disponiveis]\n",
        "y = df_model['fraudulent']\n",
        "\n",
        "print(f\"\\nFormato de X (features): {X.shape}\")\n",
        "print(f\"Formato de y (target): {y.shape}\")\n",
        "print(f\"\\nDistribui√ß√£o do target:\")\n",
        "print(y.value_counts())\n",
        "print(f\"\\nPercentual:\")\n",
        "print(y.value_counts(normalize=True) * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Normaliza√ß√£o dos Dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normaliza√ß√£o usando StandardScaler\n",
        "# Isso √© importante porque as features t√™m escalas diferentes\n",
        "# (ex: price pode ser 1000-5000, mas bedrooms pode ser 1-5)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"NORMALIZA√á√ÉO DOS DADOS\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\n‚úì Dados normalizados usando StandardScaler\")\n",
        "print(\"   - M√©dia = 0\")\n",
        "print(\"   - Desvio Padr√£o = 1\")\n",
        "print(\"\\nPrimeiras linhas dos dados normalizados:\")\n",
        "print(X_scaled.head())\n",
        "\n",
        "print(\"\\nEstat√≠sticas dos dados normalizados:\")\n",
        "print(X_scaled.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Divis√£o em Treino e Teste\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Divis√£o em conjunto de treino (80%) e teste (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, \n",
        "    test_size=0.2, \n",
        "    random_state=42,\n",
        "    stratify=y  # Mant√©m a propor√ß√£o de classes em treino e teste\n",
        ")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"DIVIS√ÉO TREINO/TESTE\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\n‚úì Divis√£o realizada: 80% treino, 20% teste\")\n",
        "print(f\"\\nConjunto de Treino:\")\n",
        "print(f\"  - X_train: {X_train.shape}\")\n",
        "print(f\"  - y_train: {y_train.shape}\")\n",
        "print(f\"\\nConjunto de Teste:\")\n",
        "print(f\"  - X_test: {X_test.shape}\")\n",
        "print(f\"  - y_test: {y_test.shape}\")\n",
        "\n",
        "print(f\"\\n\\nDistribui√ß√£o de classes no treino:\")\n",
        "print(y_train.value_counts())\n",
        "print(f\"\\nDistribui√ß√£o de classes no teste:\")\n",
        "print(y_test.value_counts())\n",
        "\n",
        "print(\"\\n‚úì Dados prontos para treinamento dos modelos!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Modelagem de Machine Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Treinamento de M√∫ltiplos Modelos\n",
        "\n",
        "Vamos treinar e avaliar 4 algoritmos diferentes de classifica√ß√£o:\n",
        "\n",
        "1. **Logistic Regression**: Modelo linear simples e interpret√°vel\n",
        "2. **Decision Tree**: Modelo baseado em √°rvore de decis√£o\n",
        "3. **Random Forest**: Ensemble de √°rvores de decis√£o\n",
        "4. **Support Vector Machine (SVM)**: Modelo que encontra o hiperplano √≥timo de separa√ß√£o\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dicion√°rio para armazenar os modelos\n",
        "modelos = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10),\n",
        "    'SVM': SVC(random_state=42, kernel='rbf')\n",
        "}\n",
        "\n",
        "# Dicion√°rio para armazenar resultados\n",
        "resultados = {}\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"TREINAMENTO DOS MODELOS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Treinar cada modelo\n",
        "for nome, modelo in modelos.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Treinando: {nome}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Treinar o modelo\n",
        "    modelo.fit(X_train, y_train)\n",
        "    \n",
        "    # Fazer previs√µes\n",
        "    y_pred_train = modelo.predict(X_train)\n",
        "    y_pred_test = modelo.predict(X_test)\n",
        "    \n",
        "    # Calcular m√©tricas\n",
        "    resultados[nome] = {\n",
        "        'modelo': modelo,\n",
        "        'y_pred_test': y_pred_test,\n",
        "        'accuracy_train': accuracy_score(y_train, y_pred_train),\n",
        "        'accuracy_test': accuracy_score(y_test, y_pred_test),\n",
        "        'precision': precision_score(y_test, y_pred_test, average='weighted', zero_division=0),\n",
        "        'recall': recall_score(y_test, y_pred_test, average='weighted', zero_division=0),\n",
        "        'f1_score': f1_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
        "    }\n",
        "    \n",
        "    print(f\"‚úì Modelo treinado!\")\n",
        "    print(f\"  - Accuracy (Treino): {resultados[nome]['accuracy_train']:.4f}\")\n",
        "    print(f\"  - Accuracy (Teste): {resultados[nome]['accuracy_test']:.4f}\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"‚úì Todos os modelos foram treinados com sucesso!\")\n",
        "print(f\"{'='*80}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Compara√ß√£o de Desempenho dos Modelos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criar tabela comparativa\n",
        "comparacao = pd.DataFrame({\n",
        "    'Modelo': list(resultados.keys()),\n",
        "    'Accuracy': [resultados[m]['accuracy_test'] for m in resultados.keys()],\n",
        "    'Precision': [resultados[m]['precision'] for m in resultados.keys()],\n",
        "    'Recall': [resultados[m]['recall'] for m in resultados.keys()],\n",
        "    'F1-Score': [resultados[m]['f1_score'] for m in resultados.keys()]\n",
        "})\n",
        "\n",
        "# Ordenar por Accuracy\n",
        "comparacao = comparacao.sort_values('Accuracy', ascending=False)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"COMPARA√á√ÉO DE DESEMPENHO DOS MODELOS\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\n\")\n",
        "print(comparacao.to_string(index=False))\n",
        "print(\"\\n\")\n",
        "\n",
        "# Identificar o melhor modelo\n",
        "melhor_modelo_nome = comparacao.iloc[0]['Modelo']\n",
        "melhor_accuracy = comparacao.iloc[0]['Accuracy']\n",
        "\n",
        "print(f\"üèÜ MELHOR MODELO: {melhor_modelo_nome}\")\n",
        "print(f\"   Accuracy: {melhor_accuracy:.4f} ({melhor_accuracy*100:.2f}%)\")\n",
        "\n",
        "# Visualiza√ß√£o comparativa\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Gr√°fico de barras das m√©tricas\n",
        "metricas_df = comparacao.set_index('Modelo')[['Accuracy', 'Precision', 'Recall', 'F1-Score']]\n",
        "metricas_df.plot(kind='bar', ax=axes[0], width=0.8)\n",
        "axes[0].set_title('Compara√ß√£o de M√©tricas por Modelo', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('Score')\n",
        "axes[0].set_xlabel('Modelo')\n",
        "axes[0].legend(loc='lower right')\n",
        "axes[0].set_ylim(0, 1.1)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "# Gr√°fico de barras apenas com Accuracy\n",
        "colors = ['green' if m == melhor_modelo_nome else 'skyblue' for m in comparacao['Modelo']]\n",
        "axes[1].bar(comparacao['Modelo'], comparacao['Accuracy'], color=colors, edgecolor='black')\n",
        "axes[1].set_title('Accuracy dos Modelos', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].set_xlabel('Modelo')\n",
        "axes[1].set_ylim(0, 1.1)\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "axes[1].set_xticklabels(comparacao['Modelo'], rotation=45, ha='right')\n",
        "\n",
        "# Adicionar valores nas barras\n",
        "for i, v in enumerate(comparacao['Accuracy']):\n",
        "    axes[1].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 An√°lise Detalhada do Melhor Modelo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
